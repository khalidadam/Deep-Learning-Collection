# -*- coding: utf-8 -*-
"""model

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IWkvIPETrunSiswM-OQ-Wfq-zVid2MXx
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision

class Block(nn.Module):
  def __init__(self, in_channel, out_channel):
    super(Block,self).__init__()
    self.in_channel = in_channel
    self.out_channel = out_channel
    self.e1 = nn.Conv2d(in_channels = in_channel, out_channels = self.out_channel, kernel_size = 3, stride = 1, padding = 1)
    self.relu = nn.ReLU(inplace = True)
    self.e2 = nn.Conv2d(in_channels = self.out_channel, out_channels = self.out_channel, kernel_size = 3, stride = 1, padding = 1)

  def forward(self, x):
    x = self.relu(self.e2(self.relu(self.e1(x))))
    return x


class UNet(nn.Module):
  def __init__(self, in_channel= 3, out_channel= 3):
    super(UNet,self).__init__()
    self.in_channel = in_channel
    self.out_channel = out_channel
    self.channels = [64,128,256,512]
    self.feat_maps = []

    # Network 
    self.encoder = nn.ModuleList()
    self.upsampler = nn.ModuleList()
    self.decoder = nn.ModuleList()
    self.pool = nn.MaxPool2d(kernel_size = 2, stride = 2)
    

    # Downsampling 
    in_channel = self.in_channel
    for channel in self.channels:
      self.encoder.append(Block(in_channel,channel))
      in_channel = channel
    
    # Bottleneck
    self.bottleneck = Block(self.channels[-1], self.channels[-1]*2)
    
    # Upsampling
    in_channel = self.channels[-1]*2
    for channel in reversed(self.channels):
      self.upsampler.append(nn.ConvTranspose2d(in_channel,channel,kernel_size = 2,stride = 2))
      self.decoder.append(Block(in_channel, channel))
      in_channel = channel

    # Output layer
    self.output_layer = nn.Conv2d(self.channels[0],self.out_channel, kernel_size = 1)

  def forward(self, x): 
    feature_maps = []

    for encoder_block in self.encoder:
      x = encoder_block(x)
      feature_maps.append(x)
      x = self.pool(x)

    x = self.bottleneck(x)
    feature_maps = feature_maps[::-1]

    for block in range(len(self.upsampler)):
      feature_map = feature_maps[block]
      x = self.upsampler[block](x)
      if feature_map.shape != x.shape:
        x = torchvision.transforms.functional.resize(x, size = feature_map.shape[2:])
      x = torch.cat((feature_map,x),dim = 1)
      x = self.decoder[block](x)
    
    return self.output_layer(x)