# -*- coding: utf-8 -*-
"""utils

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PWBnYBU6Q5GGSnN_Z_luOgOFjNb-JFCU
"""

!pip install -U torchtext==0.6
!python -q -m spacy download en
!python -q -m spacy download de

import spacy
from torchtext.data import Field, TabularDataset, BucketIterator
from torchtext.datasets import Multi30k
from torchtext.data.metrics import bleu_score

import torchtext
torchtext.__version__

spacy_eng = spacy.load('en')
spacy_de = spacy.load('de')
def tokenize_eng(text):
  return [tok.text for tok in spacy_eng.tokenizer(text)]
def tokenize_de(text):
  return [tok.text for tok in spacy_de.tokenizer(text)]

english = Field(sequential=True, use_vocab=True, tokenize=tokenize_eng, lower=True)
german = Field(sequential=True, use_vocab=True, tokenize=tokenize_de, lower=True)

train_data, validation_data, test_data = Multi30k.splits(exts=('.de', '.en'),
                                                         fields=(german, english))

# Vocabulary
english.build_vocab(train_data, max_size=10_000, min_freq=2)
german.build_vocab(train_data, max_size=10_000, min_freq=2)

train_iterator, validation_iterator, test_iterator = BucketIterator.splits(
    (train_data, validation_data,test_data),
    batch_size=128,
    device='cpu'
)
test = next(iter(train_iterator))

test.trg[]

def datasetGenerator(batch, device, max_size=10_000):
    # Spacy_language => contains specific set of rules governing certain language and its specific in tokenization, stop words or functions. 
    spacy_eng = spacy.load('en')
    spacy_de = spacy.load('de')

    # Tokenizing => breaking the sentances into list of words
    def tokenize_eng(text):
        return [tok.text for tok in spacy_eng.tokenizer(text)]
    def tokenize_de(text):
        return [tok.text for tok in spacy_de.tokenizer(text)]

    # Creating Field which converts data to tensor and overall preprocess the data like a torchvision for images
    english = Field(sequential=True, use_vocab=True, tokenize=tokenize_eng, lower=True, init_token = '<sos>', eos_token='<eos>')
    german = Field(sequential=True, use_vocab=True, tokenize=tokenize_eng, lower=True, init_token = '<sos>', eos_token='<eos>')

    # Splitting the data
    train_data, validation_data, test_data = Multi30k.splits(exts=('.de', '.en'),
                                                            fields=(german, english))
    
    # Creating vocabulary => list of words occuring in the dataset, min_freq => word needs to be used at least 2 times
    english.build_vocab(train_data, max_size=max_size, min_freq=2)
    german.build_vocab(train_data, max_size=max_size, min_freq=2)

    # Creating iterators over dataset => tensors
    train_iterator, validation_iterator, test_iterator = BucketIterator.splits(
        (train_data, validation_data,test_data),
        batch_size=batch,
        device=device
    )
    return (train_iterator, validation_iterator, test_iterator), (english, german)

(train_iterator, validation_iterator, test_iterator), (english, german) = datasetGenerator(128,'cpu')

len(german.vocab)

#test = next(iter(train_iterator))
import torch
#t = torch.swapaxes(test.trg, 0, 1)
for idx in test.trg[0]:
    print(english.vocab.itos[idx])

def bleu():
    ...