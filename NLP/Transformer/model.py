# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_8BEDzoD7kckLKC9rv4qkggqFDdQP64U
"""

import torch
import torch.nn as nn
from torch import Tensor
import torch.nn.functional as F
import math

class Embedding(nn.Module):
    " Embedding layer with scalling and dropout. "
    def __init__(
        self,
        d : int,
        vocab_size : int,
        ):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, d)

    def forward(self, x: Tensor) -> Tensor:
        return self.embedding(x)


class PE(nn.Module):
    "Implement the Positional Encoding function with dropout. "
    def __init__(
        self,
        d : int,
        p : int,
        max_len = 100
        ):    
        super().__init__()

        self.pe = torch.zeros(max_len, d)
        pos = torch.arange(0, max_len, 1).unsqueeze(1)
        div = torch.pow(10_000, 2 * torch.arange(0, d, 2) / d)
        self.pe[:, 0::2] = torch.sin(pos / div)
        self.pe[:, 1::2] = torch.cos(pos / div)

        self.dropout = nn.Dropout(p)

    def forward(self, x: Tensor) -> Tensor:
        return self.dropout(x + self.pe[:x.shape[1]])


class SelfAttention(nn.Module):
    " Multi head self-attention sub-layer followed by Add&Norm layer. "
    def __init__(
        self, 
        heads : int,
        d : int,
        p : int = 0.1
        ):

        super().__init__()

        self.heads = heads
        self.head_dim = d // heads
        self.d = d

        self.Q = nn.Linear(self.head_dim, self.head_dim)
        self.K = nn.Linear(self.head_dim, self.head_dim)
        self.V = nn.Linear(self.head_dim, self.head_dim)

        self.linear = nn.Linear(self.d, self.d)
        self.norm = nn.LayerNorm(d)
        self.dropout = nn.Dropout(p)
        
    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask=None) -> Tensor:
        batch = q.shape[0]
        q_len = q.shape[1]
        k_len = k.shape[1]
        v_len = v.shape[1]
        
        Q = self.Q(q.reshape(batch, q_len, self.heads, self.head_dim))
        K = self.K(k.reshape(batch, k_len, self.heads, self.head_dim))
        V = self.V(v.reshape(batch, v_len, self.heads, self.head_dim))

        QK = torch.einsum("bqhd, bkhd -> bhqk", [Q, K])
        scale = QK / math.sqrt(self.d)

        if mask is not None:
            scale = scale.masked_fill(mask == 0, float("-inf"))

        softmax = F.softmax(scale, dim=3)
        output = torch.einsum("bhqk, bvhd -> bqhd", [softmax, V])
        concat = output.reshape(batch, q_len, self.d)
        linear = self.linear(concat)
        addnorm = q + self.dropout(self.norm(linear))

        return addnorm

class FeedForward(nn.Module):
    " Position-wise fully conntected feed-forward network with 2 linear transformations, where first is followed by ReLU activation with Add&Norm operation."
    def __init__(
        self,
        d : int,
        dff : int,
        p : int = 0.1
        ):
        super().__init__()

        self.ff = nn.Sequential(
            nn.Linear(d, dff),
            nn.ReLU(),
            nn.Linear(dff, d)
        )

        self.norm = nn.LayerNorm(d)
        self.dropout = nn.Dropout(p)
        
    def forward(self, x: Tensor) -> Tensor:
        " Applying norm before the *add* operation empirically yields better results. "
        norm = self.norm(self.ff(x))
        return x + self.dropout(norm)

class EncoderLayer(nn.Module):
    "Encoder layer with two sub-layers multi-head attention and position-wise fully conntected feed-forward network. "
    def __init__(
        self,
        heads : int,
        d : int,
        dff : int
        ):
        super().__init__()

        self.attention = SelfAttention(heads, d)
        self.ff = FeedForward(d, dff)

    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Tensor = None) -> Tensor:
        return self.ff(self.attention(q, k, v, mask))

class DecoderLayer(nn.Module):
    "Decoder layer with three sub-layers, two multi-head attention mechanisms and position-wise fully conntected feed-forward network on the top."
    def __init__(
        self,
        heads : int,
        d : int,
        dff : int        
        ):
        super().__init__()

        self.masked_attention = SelfAttention(heads, d)
        self.enc_layer = EncoderLayer(heads, d, dff)

    def forward(self, x: Tensor, k: Tensor, v: Tensor, trg_mask: Tensor) -> Tensor:
        q = self.masked_attention(x, x, x, trg_mask)
        return self.enc_layer(q, k, v)

class EncoderDecoder(nn.Module):
    " Encoder-Decoder archiecture without Positional Encoding nor Embeddings."
    def __init__(
        self,
        heads : int,
        d : int,
        dff : int,
        N : int,
        ):
        super().__init__()

        self.enc_layer = nn.ModuleList([EncoderLayer(heads, d, dff) for _ in range(N)])
        self.dec_layer = nn.ModuleList([DecoderLayer(heads, d, dff) for _ in range(N)])

    def forward(self, src: Tensor, trg: Tensor, src_mask: Tensor, trg_mask: Tensor) -> Tensor:
        for enc in self.enc_layer:
            src = enc(src, src, src, src_mask)

        for dec in self.dec_layer:
            trg = dec(trg, src, src, trg_mask)
        
        return trg

class Classifier(nn.Module):
    " The last stage of transformer architecure where the output of decoder is passed through linear layer."
    def __init__(
        self,
        d : int,
        trg_vocab_size : int
        ):
        super().__init__()
        
        self.linear = nn.Linear(d, trg_vocab_size)

    def forward(self, x: Tensor) -> Tensor:
        return self.linear(x)

class Transformer(nn.Module):
    def __init__(
        self,
        config,
        src_vocab_size : int,
        trg_vocab_size : int,
        src_pad : int,
        trg_pad : int
        ):
        super().__init__()

        self.encdec = EncoderDecoder(config.heads, config.d, config.dff, config.N)
        self.pe = PE(config.d, config.p)
        self.src_embeddings = Embedding(config.d, src_vocab_size)
        self.trg_embeddings = Embedding(config.d, trg_vocab_size)
        self.classifier = Classifier(config.d, trg_vocab_size)

        self.src_pad = src_pad
        self.trg_pad = trg_pad

    def forward(self, src: Tensor, trg: Tensor) -> Tensor:
        src_mask = self.create_pad_mask(src, self.src_pad)
        trg_mask = self.create_att_mask(trg, self.trg_pad)

        src = self.pe(self.src_embeddings(src))
        trg = self.pe(self.trg_embeddings(trg))
        output = self.encdec(src, trg, src_mask, trg_mask)
        return self.classifier(output)

    def create_pad_mask(self, x: Tensor, pad_idx: int):
        batch, seq_len = x.shape
        mask = x != pad_idx
        return mask.reshape(batch, 1, 1, seq_len)

    def create_att_mask(self, x: Tensor, pad_idx: int):
        batch, seq_len = x.shape
        pad_mask = self.create_pad_mask(x, pad_idx)
        att_mask = torch.triu(torch.ones(seq_len, seq_len)==1).reshape(batch, 1, seq_len, seq_len)
        return att_mask & pad_mask


from model_config import Config

if __name__ == "__main__":

    src = torch.randint(0, 100, (1, 4))
    trg = torch.randint(0, 50, (1, 2))
    c = Config()
    t = Transformer(
        c,
        src_vocab_size = 100,
        trg_vocab_size = 50,
        src_pad = 1,
        trg_pad = 1
        )

    print(t(src, trg).shape)
